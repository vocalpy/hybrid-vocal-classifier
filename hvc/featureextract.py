"""
feature extraction
"""

# from standard library
import sys
import os
import glob

# from dependencies
import numpy as np
from sklearn.externals import joblib

# from hvc
from .parseconfig import parse_config
from . import features
from .utils import timestamp

SELECT_TEMPLATE = """select:
  num_replicates: None
  num_train_samples:
    start : None
    stop : None
    step : None
  num_test_samples: None

    models:"""

MODELS_TEMPLATE = """
    -
      model: {0}
      feature_list_indices: {1}
      hyperparameters:
        {2}
"""

SVM_HYPERPARAMS = """C : None
        gamma : None
"""

KNN_HYPERPARAMS = """k : None
"""

TODO_TEMPLATE = """  todo_list:
    -
      feature_file : {0}
      output_dir: {1}"""


def write_select_config(summary_ftr_file_dict,
                       summary_filename,
                       output_dir):
    """writes summary output dict from extract to a config file for select
    Only called when feature_group is a list, to save user from figuring
    out manually which features belong to which group.

    Parameters
    ----------
    summary_ftr_file_dict : dictionary
        as defined in featureextract.extract
    summary_filename : string
        name of summary feature file
    output_dir : string
        name of output directory -- assumes it will be the same as it was for extract.yml

    Returns
    -------
    None
    
    Doesn't return anything, just saves .yml file
    """

    select_config_filename = 'select.config.from_' + summary_filename + '.yml'
    with open(select_config_filename, 'w') as yml_outfile:
        yml_outfile.write(SELECT_TEMPLATE)
        for model_name, model_ID in summary_ftr_file_dict['feature_group_ID_dict'].items():
            inds = [ftr_list_ind for ftr_list_ind, grp_ID in
                    enumerate(summary_ftr_file_dict['feature_list_group_ID'])
                    if grp_ID == model_ID]
            if model_name == 'svm':
                hyperparams = SVM_HYPERPARAMS
            elif model_name == 'knn':
                hyperparams = KNN_HYPERPARAMS
            yml_outfile.write(MODELS_TEMPLATE.format(model_name,
                                                     inds,
                                                     hyperparams))
        yml_outfile.write(TODO_TEMPLATE.format(summary_filename,
                                               output_dir))


def _extract(extract_params, calling_function, make_summary_file=True):
    """helper function that loops through data dirs,
    extracts features, saves feature files, and then
    makes summary feature file.
    Put into a separate helper function so that it can
    also be used by labelpredict.predict

    Parameters
    ----------
    extract_params : dict with following key, value pairs
        data_dirs : list
            list of directories with data files
        home_dir : str
        output_dir :
        file_format : str
            {'evtaf', 'koumura'}
        feature_list : list
            list of features generated by hvc.parse.extract
        spect_params : dict
            as defined for hvc.audiofileIO.Spectrogram class
        labelset : str
            labels for syllables as a string, e.g., 'iabcdefg'
        segment_params : dict
            as defined for hvc.audiofileIO.Song and hvc.audiofileIO.segment_song
    calling_function : str
        {'extract', 'predict'}
        Flag passed to features.extract.from_file to tell it which function called it.
        If calling_function=='extract', then look for annotation files.
        with onsets, offsets, and labels.
        If calling_function=='predict', do segmentation to get onsets and offsets.
    make_summary_file : bool
        if True, combine feature files from each directory to make a summary file

    Returns
    -------
    None
    Saves output files in output dir

    """

    for data_dir in extract_params['data_dirs']:
        print('Changing to data directory: {}'.format(data_dir))
        os.chdir(extract_params['home_dir'])
        os.chdir(data_dir)

        if 'features_from_all_files' in locals():
            # from last time through loop
            # (need to re-initialize for each directory)
            del features_from_all_files

        if 'neuralnet_inputs_dict' in locals():
            del neuralnet_inputs_dict

        if extract_params['file_format'] == 'evtaf':
            songfiles_list = glob.glob('*.cbin')
        elif extract_params['file_format'] == 'koumura':
            songfiles_list = glob.glob('*.wav')

        num_songfiles = len(songfiles_list)
        all_labels = []
        all_onsets_s = []
        all_onsets_Hz = []
        all_offsets_s = []
        all_offsets_Hz = []
        songfile_IDs = []
        songfile_ID_counter = 0
        for file_num, songfile in enumerate(songfiles_list):
            print('Processing audio file {} of {}.'.format(file_num + 1, num_songfiles))
            # segment_params defined for todo_list item takes precedence over any default
            # defined for `extract` config
            extract_dict = features.extract.from_file(songfile,
                                                      extract_params['file_format'],
                                                      calling_function,
                                                      extract_params['feature_list'],
                                                      extract_params['spect_params'],
                                                      extract_params['labelset'],
                                                      extract_params['segment_params'])

            if extract_dict is None:
                # because no labels from labels_to_use were found in songfile
                continue

            all_labels.extend(extract_dict['labels'])
            all_onsets_s.extend(extract_dict['onsets_s'])
            all_onsets_Hz.extend(extract_dict['onsets_Hz'])
            all_offsets_s.extend(extract_dict['offsets_s'])
            all_offsets_Hz.extend(extract_dict['offsets_Hz'])
            songfile_IDs.extend(
                [songfile_ID_counter] * extract_dict['onsets_s'].shape[0])
            songfile_ID_counter += 1

            if 'features_arr' in extract_dict:
                if 'features_from_all_files' in locals():
                    features_from_all_files = np.concatenate((features_from_all_files,
                                                              extract_dict['features_arr']),
                                                             axis=0)
                else:
                    features_from_all_files = extract_dict['features_arr']

            if 'neuralnet_inputs_dict' in extract_dict:
                if 'neuralnet_inputs_all_files' in locals():
                    for key, val in neuralnet_inputs_all_files.items():
                        new_val = np.concatenate((neuralnet_inputs_all_files[key],
                                                  extract_dict['neuralnet_inputs_dict'][key]))
                        neuralnet_inputs_all_files[key] = new_val
                else:
                    neuralnet_inputs_all_files = extract_dict['neuralnet_inputs_dict']

        # save filename with full path
        # so hvc.convert can use full path
        # to save annotation files in same directory as original audio files
        songfiles_list = [os.path.join(os.getcwd(),
                                       songfile)
                          for songfile in songfiles_list]

        # get dir name without the rest of path so it doesn't have separators in the name
        # because those can't be in filename
        just_dir_name = os.getcwd().split(os.path.sep)[-1]
        feature_file = os.path.join(extract_params['output_dir'],
                                    'features_from_' + just_dir_name + '_created_' + timestamp())
        feature_file_dict = {
            'labels': all_labels,
            'onsets_s': np.asarray(all_onsets_s),
            'onsets_Hz': np.asarray(all_onsets_Hz),
            'offsets_s': np.asarray(all_offsets_s),
            'offsets_Hz': np.asarray(all_offsets_Hz),
            'feature_list': extract_params['feature_list'],
            'spect_params': extract_params['spect_params'],
            'segment_params': extract_params['segment_params'],
            'labelset': extract_params['labelset'],
            'file_format': extract_params['file_format'],
            'bird_ID': extract_params['bird_ID'],
            'songfile_IDs': songfile_IDs,
            'songfiles': songfiles_list
        }

        if 'features_from_all_files' in locals():
            feature_file_dict['features'] = features_from_all_files
            feature_file_dict['features_arr_column_IDs'] = extract_dict['feature_inds']

            if 'feature_list_group_ID' in extract_params:
                feature_file_dict['feature_list_group_ID'] = extract_params['feature_list_group_ID']
                feature_file_dict['feature_group_ID_dict'] = extract_params['feature_group_ID_dict']

        if 'neuralnet_inputs_all_files' in locals():
            feature_file_dict['neuralnet_inputs'] = neuralnet_inputs_all_files

        joblib.dump(feature_file_dict,
                    feature_file,
                    compress=3)

    ##########################################################
    # after looping through all data_dirs for this todo_item #
    ##########################################################

    if make_summary_file:
        print('making summary file')
        os.chdir(extract_params['output_dir'])
        summary_filename = 'summary_feature_file_created_' + timestamp()
        summary_filename_with_path = os.path.join(extract_params['output_dir'],
                                                  summary_filename)
        ftr_output_files = glob.glob('*features_from_*')
        if len(ftr_output_files) > 1:
            # make a 'summary' data file
            list_of_output_dicts = []
            summary_ftr_file_dict = {}
            for feature_file in ftr_output_files:
                feature_file_dict = joblib.load(feature_file)

                if 'labels' not in summary_ftr_file_dict:
                    summary_ftr_file_dict['labels'] = feature_file_dict['labels']
                else:
                    summary_ftr_file_dict['labels'] = \
                        summary_ftr_file_dict['labels'] + feature_file_dict['labels']

                if 'onsets_s' not in summary_ftr_file_dict:
                    summary_ftr_file_dict['onsets_s'] = feature_file_dict['onsets_s']
                else:
                    summary_ftr_file_dict['onsets_s'] = \
                        np.concatenate((summary_ftr_file_dict['onsets_s'],
                                        feature_file_dict['onsets_s']))

                if 'onsets_Hz' not in summary_ftr_file_dict:
                    summary_ftr_file_dict['onsets_Hz'] = feature_file_dict['onsets_Hz']
                else:
                    summary_ftr_file_dict['onsets_Hz'] = \
                        np.concatenate((summary_ftr_file_dict['onsets_Hz'],
                                        feature_file_dict['onsets_Hz']))

                if 'offsets_s' not in summary_ftr_file_dict:
                    summary_ftr_file_dict['offsets_s'] = feature_file_dict['offsets_s']
                else:
                    summary_ftr_file_dict['offsets_s'] = \
                        np.concatenate((summary_ftr_file_dict['offsets_s'],
                                        feature_file_dict['offsets_s']))

                if 'offsets_Hz' not in summary_ftr_file_dict:
                    summary_ftr_file_dict['offsets_Hz'] = feature_file_dict['offsets_Hz']
                else:
                    summary_ftr_file_dict['offsets_Hz'] = \
                        np.concatenate((summary_ftr_file_dict['offsets_Hz'],
                                        feature_file_dict['offsets_Hz']))

                if 'spect_params' not in summary_ftr_file_dict:
                    summary_ftr_file_dict['spect_params'] = feature_file_dict['spect_params']
                else:
                    if feature_file_dict['spect_params'] != summary_ftr_file_dict['spect_params']:
                        raise ValueError('mismatch between spect_params in {} '
                                         'and other feature files'.format(feature_file))

                if 'segment_params' not in summary_ftr_file_dict:
                    summary_ftr_file_dict['segment_params'] = feature_file_dict['segment_params']
                else:
                    if feature_file_dict['segment_params'] != summary_ftr_file_dict['segment_params']:
                        raise ValueError('mismatch between segment_params in {} '
                                         'and other feature files'.format(feature_file))

                if 'labelset' not in summary_ftr_file_dict:
                    summary_ftr_file_dict['labelset'] = feature_file_dict['labelset']
                else:
                    if feature_file_dict['labelset'] != summary_ftr_file_dict['labelset']:
                        raise ValueError('mismatch between labelset in {} '
                                         'and other feature files'.format(feature_file))

                if 'file_format' not in summary_ftr_file_dict:
                    summary_ftr_file_dict['file_format'] = feature_file_dict['file_format']
                else:
                    if feature_file_dict['file_format'] != summary_ftr_file_dict['file_format']:
                        raise ValueError('mismatch between file_format in {} '
                                         'and other feature files'.format(feature_file))

                if 'bird_ID' not in summary_ftr_file_dict:
                    summary_ftr_file_dict['bird_ID'] = feature_file_dict['bird_ID']
                else:
                    if feature_file_dict['bird_ID'] != summary_ftr_file_dict['bird_ID']:
                        raise ValueError('mismatch between bird_ID in {} '
                                         'and other feature files'.format(feature_file))

                if 'songfile_IDs' not in summary_ftr_file_dict:
                    summary_ftr_file_dict['songfile_IDs'] = feature_file_dict['songfile_IDs']
                else:
                    new_1st_ID = len(summary_ftr_file_dict['songfile_IDs'])  # works because of 0 indexing
                    tmp_songfile_IDs = [ID + new_1st_ID for ID in feature_file_dict['songfile_IDs']]
                    summary_ftr_file_dict['songfile_IDs'].extend(tmp_songfile_IDs)

                if 'songfiles' not in summary_ftr_file_dict:
                    summary_ftr_file_dict['songfiles'] = feature_file_dict['songfiles']
                else:
                    summary_ftr_file_dict['songfiles'].extend(feature_file_dict['songfiles'])

                if 'feature_list' not in summary_ftr_file_dict:
                    summary_ftr_file_dict['feature_list'] = feature_file_dict['feature_list']
                else:
                    if feature_file_dict['feature_list'] != summary_ftr_file_dict['feature_list']:
                        raise ValueError('mismatch between feature_list in {} '
                                         'and other feature files'.format(feature_file))

                # only if not-neuralnet features were used
                if 'features' in feature_file_dict:
                    if 'features' not in summary_ftr_file_dict:
                        summary_ftr_file_dict['features'] = feature_file_dict['features']
                    else:
                        summary_ftr_file_dict['features'] = np.concatenate((summary_ftr_file_dict['features'],
                                                                            feature_file_dict['features']))

                    if 'features_arr_column_IDs' not in summary_ftr_file_dict:
                        summary_ftr_file_dict['features_arr_column_IDs'] = feature_file_dict['features_arr_column_IDs']
                    else:
                        if any(feature_file_dict['features_arr_column_IDs'] !=
                                       summary_ftr_file_dict['features_arr_column_IDs']):
                            raise ValueError('mismatch between features_arr_column_IDs in {} '
                                             'and other feature files'.format(feature_file))

                    if 'feature_list_group_ID' in extract_params:
                        if 'feature_list_group_ID' not in summary_ftr_file_dict:
                            summary_ftr_file_dict['feature_list_group_ID'] = feature_file_dict['feature_list_group_ID']
                        else:
                            if feature_file_dict['feature_list_group_ID'] != \
                                    summary_ftr_file_dict['feature_list_group_ID']:
                                raise ValueError('mismatch between feature_list_group_ID in {} '
                                                 'and other feature files'.format(feature_file))

                        if 'feature_group_ID_dict' not in summary_ftr_file_dict:
                            summary_ftr_file_dict['feature_group_ID_dict'] = \
                                feature_file_dict['feature_group_ID_dict']
                        else:
                            if feature_file_dict['feature_group_ID_dict'] != \
                                    summary_ftr_file_dict['feature_group_ID_dict']:
                                raise ValueError('mismatch between feature_group_ID_dict in {} '
                                                 'and other feature files'.format(feature_file))

                # if extracting inputs for neuralnets
                if 'neuralnet_inputs' in feature_file_dict:
                    if 'neuralnet_inputs' not in summary_ftr_file_dict:
                        summary_ftr_file_dict['neuralnet_inputs'] = \
                            feature_file_dict['neuralnet_inputs']
                    else:
                        for key, val in summary_ftr_file_dict['neuralnet_inputs'].items():
                            newval = np.concatenate((summary_ftr_file_dict['neuralnet_inputs'][key],
                                                     feature_file_dict['neuralnet_inputs'][key]))
                            summary_ftr_file_dict['neuralnet_inputs'][key] = newval

            joblib.dump(summary_ftr_file_dict,
                        summary_filename)

        else:  # if only one feature_file
            os.rename(ftr_output_files[0],
                      summary_filename)
            summary_ftr_file_dict = joblib.load(summary_filename)

        if 'feature_list_group_ID' in summary_ftr_file_dict:
            write_select_config(summary_ftr_file_dict,
                                summary_filename,
                                extract_params['output_dir'])


def extract(config_file):
    """main function that runs feature extraction.
    Does not return anything, just runs through directories specified in config_file
    and extracts features.

    Parameters
    ----------
    config_file : string
        filename of YAML file that configures feature extraction    
    """

    extract_config = parse_config(config_file, 'extract')
    print('Parsed extract config.')

    home_dir = os.getcwd()

    todo_list = extract_config['todo_list']
    for ind, todo in enumerate(todo_list):

        print('Completing item {} of {} in to-do list'.format(ind+1, len(todo_list)))
        file_format = todo['file_format']
        if file_format == 'evtaf':
            if 'evfuncs' not in sys.modules:
                from . import evfuncs
        elif file_format == 'koumura':
            if 'koumura' not in sys.modules:
                from . import koumura

        output_dir = 'extract_output_' + timestamp()
        output_dir_with_path = os.path.join(todo['output_dir'], output_dir)
        if not os.path.isdir(output_dir_with_path):
            os.mkdir(output_dir_with_path)

        extract_params = {
            'bird_ID': todo['bird_ID'],
            'feature_list': todo['feature_list'],
            'output_dir': output_dir_with_path,
            'home_dir': home_dir,
            'data_dirs': todo['data_dirs'],
            'labelset': todo['labelset'],
            'file_format': todo['file_format']
        }

        # segment_params defined for todo_list item takes precedence over any default
        # defined for `extract` config
        if 'segment_params' in todo:
            extract_params['segment_params'] = todo['segment_params']
        else:
            extract_params['segment_params'] = extract_config['segment_params']

        if 'spect_params' in todo:
            extract_params['spect_params'] = todo['spect_params']
        else:
            extract_params['spect_params'] = extract_config['spect_params']

        if 'feature_list_group_ID' in todo:
            extract_params['feature_list_group_ID'] = todo['feature_list_group_ID']
            extract_params['feature_group_ID_dict'] = todo['feature_group_ID_dict']

        _extract(extract_params, calling_function='extract')

        os.chdir(home_dir)
